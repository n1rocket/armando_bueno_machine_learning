{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos Wrapper y Embedded\n",
    "\n",
    "En este notebook se revisarán los conceptos de:\n",
    "\n",
    "1. Métodos *wrapper*\n",
    "2. Métodos *embedded*\n",
    "\n",
    "Primero cargamos librerías y funciones necesarias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Métodos Wrapper\n",
    "\n",
    "Estos métodos utilizan un algoritmo de machine learning como caja negra para rankear distintos subconjuntos de variables de acuerdo a su capacidad predictiva. Normalmente se usan en mediante procedimientos hacia delante/detrás en combinación con validación cruzada.\n",
    "\n",
    "### *The wrong and right way to do cross-validation*\n",
    "\n",
    "Este ejemplo ha sido inspirado en *7.10.2 The Wrong and Right Way to Do Cross-validation* del libro\n",
    "\"The Elements of Statistical Learning\". Hastie, Tibshirani, Friedman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "\n",
    "# This is the WRONG way\n",
    "\n",
    "np.random.seed(0)\n",
    "score = []\n",
    "\n",
    "for i in range(500): # This is run for a number of experiments. Montecarlo simulation\n",
    "    \n",
    "    # Create toy example\n",
    "    N = 50\n",
    "    y = np.concatenate([-1*np.ones(int(N/2)),np.ones(int(N/2))]) # target\n",
    "    X = np.random.randn(N, 5000)                       # predictors are random variables!!\n",
    "    \n",
    "    # Note here, the ranking and selection is performed outside the CV loop\n",
    "    f_test, _ = f_classif(X, y)\n",
    "    f_test /= np.max(f_test)\n",
    "    \n",
    "    \n",
    "    ranking = np.argsort(f_test)[::-1] \n",
    "    selected = ranking[0:50]\n",
    "    Xs = X[:,selected]\n",
    "    \n",
    "    # 1-neighbor classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=1)\n",
    "    \n",
    "    # 5-fold CV\n",
    "    kf  = KFold(n_splits=5, shuffle = True)\n",
    "    score_i = []\n",
    "    \n",
    "    for train, validation in kf.split(Xs):\n",
    "        knn.fit(Xs[train,:],y[train])\n",
    "        accuracy = knn.score(Xs[validation,:],y[validation])\n",
    "        score_i.append(accuracy) \n",
    "    \n",
    "    score.append(np.mean(score_i))\n",
    "\n",
    "print(\"Error rate (%): \" + str((1-np.mean(score))*100))\n",
    "\n",
    "plt.hist(score)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the RIGHT way\n",
    "\n",
    "np.random.seed(0)\n",
    "score = []\n",
    "\n",
    "for i in range(500): # This runs for a number of experiments. Montecarlo simulation\n",
    "    \n",
    "    # Create toy example\n",
    "    N = 50\n",
    "    y = np.concatenate([-1*np.ones(int(N/2)),np.ones(int(N/2))]) # target\n",
    "    X = np.random.randn(N, 5000)                       # predictors, again random variables\n",
    "    \n",
    "    # 1-neighbor classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=1)\n",
    "    \n",
    "    # 5-fold CV\n",
    "    kf  = KFold(n_splits=5, shuffle = True)\n",
    "    score_i = []\n",
    "    \n",
    "    for train, validation in kf.split(X):\n",
    "        \n",
    "        # Note here, the ranking and selection is performed inside the CV loop\n",
    "        f_test, _ = f_classif(X[train,:], y[train])\n",
    "        f_test /= np.max(f_test)\n",
    "        ranking = np.argsort(f_test)[::-1] \n",
    "        selected = ranking[0:50]\n",
    "        \n",
    "        Xs = X[:,selected]\n",
    "        \n",
    "        knn.fit(Xs[train,:],y[train])\n",
    "        accuracy = knn.score(Xs[validation,:],y[validation])\n",
    "        \n",
    "        score_i.append(accuracy) \n",
    "    \n",
    "    score.append(np.mean(score_i))\n",
    "\n",
    "print(\"Error rate (%): \" + str((1-np.mean(score))*100))\n",
    "\n",
    "plt.hist(score)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Métodos embedded\n",
    "\n",
    "Vamos a trabajar directamente sobre la base de datos  de viviendas [House Sales in King COunty, USA](https://www.kaggle.com/harlfoxem/housesalesprediction), así que lo primero es cargar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos datos\n",
    "house_data = pd.read_csv(\"./data/kc_house_data.csv\") # cargamos fichero\n",
    "\n",
    "# Eliminamos las columnas id y date \n",
    "house_data = house_data.drop(['id','date'], axis=1)\n",
    "\n",
    "# convertir las variables en pies al cuadrado en metros al cuadrado \n",
    "feetFeatures = ['sqft_living','sqft_lot','sqft_above','sqft_basement','sqft_living15','sqft_lot15']\n",
    "house_data[feetFeatures] = house_data[feetFeatures].apply(lambda x: x * 0.3048 * 0.3048)\n",
    "\n",
    "# renombramos\n",
    "house_data.columns = ['price','bedrooms','bathrooms','sqm_living','sqm_lot','floors','waterfront','view','condition',\n",
    "                      'grade','sqm_above','sqm_basement','yr_built','yr_renovated','zip_code','lat','long',\n",
    "                      'sqm_living15','sqm_lot15']\n",
    "\n",
    "# añadimos las nuevas variables\n",
    "house_data['years']            = 2015 - house_data['yr_built']\n",
    "#house_data['bedrooms_squared'] = house_data['bedrooms'].apply(lambda x: x**2)\n",
    "#house_data['bed_bath_rooms']   = house_data['bedrooms']*house_data['bathrooms']\n",
    "#house_data['log_sqm_living']   = house_data['sqm_living'].apply(lambda x: np.log(x))\n",
    "#house_data['lat_plus_long']    = house_data['lat']*house_data['long']\n",
    "\n",
    "house_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertimos el DataFrame al formato necesario para scikit-learn\n",
    "data = house_data.values\n",
    "\n",
    "y = data[:,0:1]     # nos quedamos con la 1ª columna, price\n",
    "X = data[:,1:]      # nos quedamos con el resto\n",
    "\n",
    "feature_names = house_data.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "\n",
    "# Dividimos los datos en entrenamiento y test (80 training, 20 test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state = 2)\n",
    "\n",
    "print('Datos entrenamiento: ', X_train.shape)\n",
    "print('Datos test: ', X_test.shape)\n",
    "\n",
    "# Escalamos (con los datos de train)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "XtrainScaled = scaler.transform(X_train)\n",
    "XtestScaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, utilizamos la capacidad de Lasso para seleccionar variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "alpha_vector = np.logspace(-1,10,50)\n",
    "param_grid = {'alpha': alpha_vector }\n",
    "grid = GridSearchCV(Lasso(), scoring= 'neg_mean_squared_error', param_grid=param_grid, cv = 10)\n",
    "grid.fit(XtrainScaled, y_train)\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"best parameters: {}\".format(grid.best_params_))\n",
    "\n",
    "#-1 porque es negado\n",
    "scores = -1*np.array(grid.cv_results_['mean_test_score'])\n",
    "plt.semilogx(alpha_vector,scores,'-o')\n",
    "plt.xlabel('alpha',fontsize=16)\n",
    "plt.ylabel('5-Fold MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "alpha_optimo = grid.best_params_['alpha']\n",
    "lasso = Lasso(alpha = alpha_optimo).fit(XtrainScaled,y_train)\n",
    "\n",
    "ytrainLasso = lasso.predict(XtrainScaled)\n",
    "ytestLasso  = lasso.predict(XtestScaled)\n",
    "mseTrainModelLasso = mean_squared_error(y_train,ytrainLasso)\n",
    "mseTestModelLasso = mean_squared_error(y_test,ytestLasso)\n",
    "\n",
    "print('MSE Modelo Lasso (train): %0.3g' % mseTrainModelLasso)\n",
    "print('MSE Modelo Lasso (test) : %0.3g' % mseTestModelLasso)\n",
    "\n",
    "#print('RMSE Modelo Lasso (train): %0.3g' % np.sqrt(mseTrainModelLasso))\n",
    "#print('RMSE Modelo Lasso (test) : %0.3g' % np.sqrt(mseTestModelLasso))\n",
    "\n",
    "w = lasso.coef_\n",
    "for f,wi in zip(feature_names,w):\n",
    "    print(f,wi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(house_data.price, bins=30)\n",
    "#plt.show()\n",
    "\n",
    "#plt.hist(np.log10(house_data.price), bins=30)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puede apreciarse, con este algoritmo hemos conseguido \"desactivar\" algunas variables (*sqm_lot*, *sqm_basement*, *long*, *bedrooms_squared*, *bed_bath_rooms*), lo que mejora la interpretabilidad del modelo a costa de aumentar ligeramente el error en test con respecto al método Ridge.\n",
    "\n",
    "No obstante, siguen apareciendo algunas incoherencias con respecto al valor de los coeficientes, como por ejemplo el asociado a la variable *bedrooms* que tiene valor negativo. \n",
    "\n",
    "Si aumentamos el parámetro de regularización y observamos los resultados obtenidos hemos aumentado el error, pero a cambio:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tenemos un modelo más sencillo (menos variables al haber muchos coeficientes nulos) y por tanto, menos susceptible a sufrir overfitting\n",
    "2. Mejoramos la interpretabilidad del modelo, las variables supervivientes (con coeficiente distintos de cero) parecen concordar con nuestra intuición sobre problema a resolver\n",
    "\n",
    "Y una vez llegados a este punto, ¿qué podemos hacer? El error de entrenamiento/validación y test es similar, pero todavía es muy alto, así que: \n",
    "\n",
    "1. Se podrían definir nuevas variables que nos ayuden a mejorar el error de predicción (mse)\n",
    "2. Jugar con el parámetro de regularización, para mantener el compromiso entre sencillez/interpretabilidad del modelo y error de predicción (MSE).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
