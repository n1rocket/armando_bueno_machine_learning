{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Árboles de decisión y regresión\n",
    "\n",
    "Vamos a analizar el funcionamiento de los [árboles de decisión](http://scikit-learn.org/stable/modules/tree.html) mediante ejemplos sencillos. \n",
    "\n",
    "## Contenidos\n",
    "\n",
    "1. Árboles de clasificación sobre ejemplos sintéticos\n",
    "2. Árboles de clasificación sobre ejemplo realista\n",
    "3. Árboles de regresión\n",
    "\n",
    "## Librerías y funciones\n",
    "\n",
    "Lo primero es cargar las librerías y funciones necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos una función para representar el resultado del ajuste\n",
    "def plot_decision_boundary(X,y,h,model):\n",
    "    \n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = .05  # step size in the mesh\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    model = model.fit(X,y)\n",
    "    Zd = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Zd = Zd.reshape(xx.shape)\n",
    "    \n",
    "    Zp = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1] \n",
    "    Zp = Zp.reshape(xx.shape)\n",
    "    \n",
    "    # Error de clasificación\n",
    "    ypred = model.predict(X)\n",
    "    acc = accuracy_score(y,ypred)\n",
    "    \n",
    "    plt.figure(1, figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright)\n",
    "    plt.axis([x_min, x_max, y_min, y_max])\n",
    "    plt.contour(xx, yy, Zd, levels=[0], linewidths=2)\n",
    "    plt.contourf(xx, yy, Zd,cmap=cm, alpha=.5)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "    plt.ylabel(\"$x_2$\", fontsize=16)\n",
    "    msg = 'FRONTERA DECISION\\n Acc: %0.2g' % acc\n",
    "    plt.title(msg)\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright)\n",
    "    plt.axis([x_min, x_max, y_min, y_max])\n",
    "    plt.contour(xx, yy, Zp, levels=[0], linewidths=2)\n",
    "    plt.contourf(xx, yy, Zp,cmap=cm, alpha=.5)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "    plt.ylabel(\"$x_2$\", fontsize=16)\n",
    "    msg = 'PROBABILIDAD\\n Acc: %0.2g' % acc\n",
    "    plt.title(msg)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Ejemplos sintéticos en clasificación\n",
    "\n",
    "Trabajaremos con los ejemplos de los Notebooks anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo1\n",
    "ejemplo1 = pd.read_csv(\"./data/ex2data1.txt\", sep=\",\", header=None, names=['x1', 'x2','label'])\n",
    "\n",
    "# ejemplo2\n",
    "ejemplo2 = pd.read_csv(\"./data/ex2data2.txt\", sep=\",\", header=None, names=['x1', 'x2','label'])\n",
    "\n",
    "# ejemplo 3: Problema XOR \n",
    "np.random.seed(0)\n",
    "\n",
    "# -- parameters\n",
    "N     = 800\n",
    "mu    = 1.5      # Cambia este valor\n",
    "sigma = 1      # Cambia este valor\n",
    "\n",
    "# variables auxiliares\n",
    "unos = np.ones(int(N/4))\n",
    "random4 = sigma*np.random.randn(int(N/4),1)\n",
    "random2 = sigma*np.random.randn(int(N/2),1)\n",
    "\n",
    "# -- features\n",
    "y3 = np.concatenate([-1*unos,       unos,          unos,         -1*unos]) \n",
    "X1 = np.concatenate([-mu + random4, mu + random4, -mu + random4, mu + random4])\n",
    "X2 = np.concatenate([+mu + random2,               -mu + random2])\n",
    "X3 = np.hstack((X1,X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.scatter(ejemplo1['x1'], ejemplo1['x2'], c=ejemplo1['label'], cmap=cm_bright)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$x_2$\", fontsize=16)\n",
    "plt.title('Ejemplo 1')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(ejemplo2['x1'], ejemplo2['x2'], c=ejemplo2['label'], cmap=cm_bright)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$x_2$\", fontsize=16)\n",
    "plt.title('Ejemplo 2')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(X3[:,0], X3[:,1], c=y3, cmap=cm_bright)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$x_2$\", fontsize=16)\n",
    "plt.title('Ejemplo 3')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo 1\n",
    "\n",
    "Vamos a entrenar un árbol de decisión sobre el ejemplo 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# preparamos los datos\n",
    "data1 = ejemplo1.values\n",
    "X1 = data1[:,0:2]\n",
    "y1 = data1[:,-1]\n",
    "\n",
    "# creamos el modelo y ajustamos\n",
    "treeModel1 = DecisionTreeClassifier()\n",
    "treeModel1.fit(X1, y1)\n",
    "\n",
    "plot_decision_boundary(X1,y1,0.05,treeModel1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO: Entrena un árbol de decisión sobre los ejemplos 2 y 3. Visualiza el resultado y coméntalo.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejemplo 2: your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejemplo 3: your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la vista de los ejemplos anteriores, podemos comprobar varias cosas:\n",
    "\n",
    "1) Seguramente estamos cometiendo overfitting, porque las fronteras de separación son altamente complejas, ¿cómo podemos controlar la complejidad de un árbol?\n",
    "\n",
    "2) Las prestaciones las estamos midiendo sobre el conjunto de entrenamiento, por lo que no sabemos el alcance real que tienen estos algoritmos.\n",
    "\n",
    "Vayamos por partes. Sobre 1)\n",
    "\n",
    "\n",
    "Los árboles de decisión tienen varios parámetros para controlar la complejidad del mismo (véase la [documentación](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)). Normalmente, los parámetros más relevantes para controlar la complejidad son:\n",
    "\n",
    "* **profundidad** del árbol (*max_depth*): Si utilizamos un árbol de profundidad 1, sólo podremos dividir el espacio en dos (1 decisión). Si utilizamos un árbol de profundidad 2, tomaremos 4 decisiones, y así sucesivamente. Por tanto, con la profundidad controlamos la complejidad del árbol de decisión, y este parámetro sirve como herramienta para **regularizar* el algoritmo:\n",
    "\n",
    "    - Un árbol de poca profundidad (poco complejo), tendrá menor riesgo de sufrir overfitting a costa de, potencialmente, incurrir en más error de clasificación.\n",
    "\n",
    "    - Un árbol de mucha profundidad (muy complejo), tendrá mayor riesgo de sufrir overfitting a costa de, potencialmente, mejorar el error de clasificación.\n",
    "\n",
    "* **Número mínimo de muestras en una hoja** (*min_samples_leaf*). Podemos forzar a que una hoja tenga un número mínimo de muestras en cada hoja, de tal forma que:\n",
    "    - Un árbol con un *min_samples_leaf* elevado, tendrá complejidad menor que un árbol con *min_samples_leaf* pequeño.\n",
    "    \n",
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO: Varía los parámetros *max_depth* y *min_samples_leaf* (de forma independiente y después conjuntamente) y comprueba el resultado, ¿coincide con tu intuición?\n",
    "</div>\n",
    "\n",
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO: Calcule el valor óptimo de *max_depth* para el ejemplo 3, ¿cuáles son las prestaciones del algoritmo para este ejemplo?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Árboles de clasificación sobre ejemplo realista\n",
    "\n",
    "Vamos a trabajar sobre un **problema multiclase** de clasificación de frutas a partir de sus propiedades. También abordaremos la visualización de un árbol de decisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits = pd.read_table('./data/fruit_data_with_colors.txt')\n",
    "print(fruits.shape)\n",
    "\n",
    "fruits.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# esto se puede hacer con un label encoder\n",
    "lookup_fruit_name = dict(zip(fruits.fruit_label.unique(), fruits.fruit_name.unique()))   \n",
    "lookup_fruit_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# preparamos los datos\n",
    "X = fruits[['height', 'width', 'mass', 'color_score']].values\n",
    "y = fruits['fruit_label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)\n",
    "\n",
    "print('Datos train: ', X_train.shape)\n",
    "print('Datos test:  ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "# creamos el modelo y ajustamos\n",
    "treeModel = DecisionTreeClassifier()\n",
    "treeModel.fit(X_train, y_train)\n",
    "\n",
    "y_test_predicted = treeModel.predict(X_test)\n",
    "print(treeModel.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizamos el árbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para esto hace falta graphviz. En Linux (y Mac, supongo) vale con pip install graphviz.\n",
    "# En Anaconda podéis buscar \"graphviz\" en la GUI e instalar los 4 paquete que os aparecen.\n",
    "# En Windows puede dar problemas con Anaconda, así que si os falla bajadlo de aquí: \n",
    "# https://graphviz.gitlab.io/_pages/Download/Download_windows.html\n",
    "# y luego añadid la ruta del ejecutable a vuestra variable de entorno PATH \n",
    "# (será algo similar a C:\\Program Files (x86)\\Graphviz2.38\\bin)\n",
    "\n",
    "import graphviz\n",
    "\n",
    "dot_data = tree.export_graphviz(treeModel, out_file=None, \n",
    "                         feature_names=['height', 'width', 'mass', 'color_score'],  \n",
    "                         class_names=['apple','mandarin','orange','lemon'],  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)\n",
    "graph = graphviz.Source(dot_data) \n",
    "\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la vista de la figura, observamos que:\n",
    "\n",
    "1) Comenzamos con un Gini elevado ¿cuál es el máximo valor para este problema? y a medida que aumentamos la profundidad el valor Gini diminuye, hasta que en todas las hojas es 0.\n",
    "\n",
    "2) Es un árbol de profundidad 4.\n",
    "\n",
    "3) Todas las muestras de entrenamiento están bien clasificadas\n",
    "\n",
    "4) La variable *color_score* se subdivide consigo misma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"mass\",\"width\",\"height\",\"color_score\"]\n",
    "test_sample = 3\n",
    "\n",
    "print('La muestra de test con etiqueta \"{0}\" y atributos: '.format(lookup_fruit_name[y_test[test_sample]]))\n",
    "for i,f in enumerate(features):\n",
    "    print(' ',f,':',X_test[test_sample,i])\n",
    "\n",
    "print(\"ha sido clasificada como: '{0}' \".format(lookup_fruit_name[y_test_predicted[test_sample]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO: Para el ejemplo de la celda anterior, ¿puedes seguir el camino de la decisión?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Árboles de regresión\n",
    "\n",
    "Vamos a aplicar árboles de regresión sobre nuestro ejemplo de regresión sintético:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 10\n",
    "N_test  = 100\n",
    "\n",
    "# función verdadera g(x)\n",
    "x = np.linspace(0,1,N_test)\n",
    "g_x = np.cos(1.5*np.pi*x)\n",
    "\n",
    "# proceso y\n",
    "np.random.seed(0) # para asegurar reproducibilidad\n",
    "epsilon = np.random.randn(N_test) * 0.2\n",
    "y = g_x + epsilon\n",
    "\n",
    "# Datos: D = {x_i,y_i}, obtenemos una muestra\n",
    "idx = np.random.randint(0,N_test,N_train)\n",
    "x_i = x[idx]\n",
    "y_i = y[idx]\n",
    "\n",
    "# YOUR CODE HERE: dibuje la función g(x), y el conjunto de datos x_i,y_i\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "X_train = x_i.reshape(-1, 1)\n",
    "y_train = y_i\n",
    "X_test  = x.reshape(-1, 1)\n",
    "\n",
    "regTree = DecisionTreeRegressor(max_depth=2)\n",
    "regTree.fit(X_train,y_train)\n",
    "\n",
    "y_hat = regTree.predict(X_test)\n",
    "\n",
    "# error\n",
    "error_test = np.mean(np.power(y - y_hat,2)) \n",
    "\n",
    "\n",
    "plt.plot(x,g_x,'r',label='$y$')\n",
    "plt.plot(x_i,y_i,'b.',label='$y_i$')\n",
    "plt.plot(x,y_hat,'g',label='$\\hat{y}$')\n",
    "plt.title('MSE:%.2f'%error_test)\n",
    "plt.legend()\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((-2, 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO: Modifica los valores de *max_depth* y observa el resultado, ¿concuerda con tu intuición?\n",
    "</div>\n",
    "\n",
    "<div class = \"alert alert-success\">\n",
    "EJERCICIO: Representa el árbol entrenado\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
