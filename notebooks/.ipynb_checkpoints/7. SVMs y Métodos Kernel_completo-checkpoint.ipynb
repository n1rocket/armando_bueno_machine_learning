{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM y Métodos Kernel\n",
    "\n",
    "En este Notebook vamos a poner a prueba el algoritmo de SVMs, tanto en problemas de clasificación como de regresión. Adicionalmente, se analizará el funcionamiento de otros métodos que utilizan el concepto de [Kernel](https://en.wikipedia.org/wiki/Kernel_method). \n",
    "\n",
    "## Contenidos\n",
    "\n",
    "1. SVMs en problemas de clasificación\n",
    "    1. Ejemplos sintéticos\n",
    "    2. Ejemplo realista ([Pima Indian Diabetes dataset](https://www.kaggle.com/uciml/pima-indians-diabetes-database))\n",
    "    3. Recursive Feature Elimination (RFE)\n",
    "2. SVMs en problemas de regresión\n",
    "    1. Ejemplo sintético\n",
    "    2. Ejemplo realista ([House Sales in King COunty, USA](https://www.kaggle.com/harlfoxem/housesalesprediction))\n",
    "3. Otros métodos Kernel\n",
    "    1. Ridge Kernel Regression\n",
    "    2. Kernel PCA\n",
    "\n",
    "## Librerías y funciones\n",
    "\n",
    "Lo primero es cargar las librerías y funciones necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos librerías necesarias\n",
    "import numpy  as np  \n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt # para dibujar\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos una función para representar el resultado del ajuste\n",
    "def plot_decision_boundary_svm(X,y,h,model):\n",
    "    \n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = .05  # step size in the mesh\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Zd = model.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Zd = Zd.reshape(xx.shape)\n",
    "    \n",
    "    Zp = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1] \n",
    "    Zp = Zp.reshape(xx.shape)\n",
    "    \n",
    "    # Error de clasificación\n",
    "    ypred = model.predict(X)\n",
    "    acc = accuracy_score(y,ypred)\n",
    "    \n",
    "    plt.figure(1, figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=100,c='k', facecolors='none')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright)\n",
    "    plt.axis([x_min, x_max, y_min, y_max])\n",
    "    plt.contour(xx, yy, Zd, colors=['k', 'k', 'k'],linestyles=['--', '-', '--'], levels=[-1, 0, 1])\n",
    "    plt.contourf(xx, yy, Zd>0,cmap=cm, alpha=.5)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "    plt.ylabel(\"$x_2$\", fontsize=16)\n",
    "    msg = 'FRONTERA DECISION\\n Acc: %0.2g' % acc\n",
    "    plt.title(msg)\n",
    "        \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=100,c='k', facecolors='none')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright)\n",
    "    plt.axis([x_min, x_max, y_min, y_max])\n",
    "    plt.contour(xx, yy, Zd, colors=['k', 'k', 'k'],linestyles=['--', '-', '--'], levels=[-1, 0, 1])\n",
    "    plt.contourf(xx, yy, Zp,cmap=cm, alpha=.5)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "    plt.ylabel(\"$x_2$\", fontsize=16)\n",
    "    msg = 'PROBABILIDAD\\n Acc: %0.2g' % acc\n",
    "    plt.title(msg)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SVMs en problemas de clasificación\n",
    "\n",
    "Comenzaremos analizando las máquinas de vectores soporte en clasificación, en ocasiones denominadas *Support Vector Classifiers*. Como hemos hecho en notebooks anteriores, primero probamos sobre ejemplos sintéticos\n",
    "\n",
    "## 1.1 Ejemplos sintéticos\n",
    "\n",
    "Cargamos y representamos nuestros ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejemplo1\n",
    "ejemplo1 = pd.read_csv(\"./data/ex2data1.txt\", sep=\",\", header=None, names=['x1', 'x2','label'])\n",
    "\n",
    "# ejemplo2\n",
    "ejemplo2 = pd.read_csv(\"./data/ex2data2.txt\", sep=\",\", header=None, names=['x1', 'x2','label'])\n",
    "\n",
    "# ejemplo 3: Problema XOR \n",
    "np.random.seed(0)\n",
    "\n",
    "# -- parameters\n",
    "N     = 800\n",
    "mu    = 1.5      # Cambia este valor\n",
    "sigma = 1      # Cambia este valor\n",
    "\n",
    "# variables auxiliares\n",
    "unos = np.ones(int(N/4))\n",
    "random4 = sigma*np.random.randn(int(N/4),1)\n",
    "random2 = sigma*np.random.randn(int(N/2),1)\n",
    "\n",
    "# -- features\n",
    "y3 = np.concatenate([-1*unos,       unos,          unos,         -1*unos]) \n",
    "X1 = np.concatenate([-mu + random4, mu + random4, -mu + random4, mu + random4])\n",
    "X2 = np.concatenate([+mu + random2,               -mu + random2])\n",
    "X3 = np.hstack((X1,X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.scatter(ejemplo1['x1'], ejemplo1['x2'], c=ejemplo1['label'], cmap=cm_bright)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$x_2$\", fontsize=16)\n",
    "plt.title('Ejemplo 1')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(ejemplo2['x1'], ejemplo2['x2'], c=ejemplo2['label'], cmap=cm_bright)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$x_2$\", fontsize=16)\n",
    "plt.title('Ejemplo 2')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(X3[:,0], X3[:,1], c=y3, cmap=cm_bright)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$x_2$\", fontsize=16)\n",
    "plt.title('Ejemplo 3')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Ejemplo 1\n",
    "# preparamos los datos\n",
    "data1 = ejemplo1.values\n",
    "X1 = data1[:,0:2]\n",
    "y1 = data1[:,-1]\n",
    "\n",
    "# creamos el modelo y ajustamos\n",
    "svmModel1= SVC(kernel='linear', probability = True)\n",
    "svmModel1.fit(X1,y1)\n",
    "\n",
    "plot_decision_boundary_svm(X1,y1,0.05,svmModel1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde hemos resaltado los **vectores soporte**, la **frontera de separación** y el **margen**. \n",
    "\n",
    "Como el problema no es linealmente separable, observamos errores de clasificación que caen tanto dentro del margen, como al otro lado de la frontera de separación correspondiente. \n",
    "\n",
    "Sabemos que podemos permitir ciertos errores dentro del margen, y los penalizamos con un coste $C$. \n",
    "\n",
    "* Si tenemos un valor elevado del coste $C$, estaremos penalizando mucho los errores, y por tanto se obtienen fronteras más ajustadas (mayor complejidad, mayor riesgo de overfitting, potenciales mejores prestaciones). \n",
    "\n",
    "* De otro lado, si tenemos un valor pequeño del coste $C$, no daremos mucha importancia a los errores, y por tanto se obtienen fronteras menos ajustadas (menor complejidad, menor riesgo de overfitting, potenciales peores prestaciones). \n",
    "\n",
    "Se puede modificar el coste $C$ mediante el parámetro de mismo nombre en scikit-learn. Por defecto, $C=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmModel1= SVC(kernel='linear',probability=True,C = 0.0001) # PIENSA ANTES COMO SERÁ EL RESULTADO!\n",
    "svmModel1.fit(X1,y1)\n",
    "\n",
    "plot_decision_boundary_svm(X1,y1,0.05,svmModel1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmModel1= SVC(kernel='linear',probability=True,C = 1e5) # PIENSA ANTES COMO SERÁ EL RESULTADO!\n",
    "svmModel1.fit(X1,y1)\n",
    "\n",
    "plot_decision_boundary_svm(X1,y1,0.05,svmModel1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplos 2 y 3\n",
    "\n",
    "Comenzamos por el ejemplo 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparamos los datos\n",
    "data2 = ejemplo2.values\n",
    "X2 = data2[:,0:2]\n",
    "y2 = data2[:,-1]\n",
    "\n",
    "# creamos el modelo\n",
    "svmModel2= SVC(kernel='linear',probability=True)\n",
    "svmModel2.fit(X2,y2)\n",
    "\n",
    "plot_decision_boundary_svm(X2,y2,0.05,svmModel2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos comprobar, un kernel lineal no nos sirve para generar una frontera de separación no lineal, así que tenemos que utilizar otros kernels.\n",
    "\n",
    "<div class = \"alert alert-success\">\n",
    "**EJERCICIO:** Echa un vistazo a la documentación para conocer cómo entrenar un algoritmo [SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) con diferentes Kernels. Prueba entonces distintos Kernels con distintos valores de sus parámetros libres\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polinómico\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "**EJERCICIO:** Entrena una SVM sobre el ejemplo 3.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "**EJERCICIO**: Suponiendo un kernel RBD, calcule el valor óptimo de *C* y *gamma* para el ejemplo 3, ¿cuáles son las prestaciones del algoritmo para este ejemplo?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Paso 1:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X3, y3, test_size = 0.3, shuffle = True, random_state = 0)\n",
    "\n",
    "# Paso 2:\n",
    "vectorC = np.logspace(-3, 3, 21)\n",
    "vectorG = np.logspace(-5, 1, 21)\n",
    "\n",
    "param_grid = {'C': vectorC,\n",
    "              'gamma':vectorG}\n",
    "\n",
    "\n",
    "# TU CODIGO AQUI\n",
    "\n",
    "\n",
    "\n",
    "######\n",
    "\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"best parameters: {}\".format(grid.best_params_))\n",
    "\n",
    "# Mostramos prestaciones en CV\n",
    "scores = grid.cv_results_['mean_test_score'].reshape(len(vectorC),len(vectorG))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(scores, interpolation='nearest', vmin= 0.6, vmax=0.9)\n",
    "plt.xlabel('log(gamma)')\n",
    "plt.ylabel('log(C)')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(vectorG)), np.log10(vectorG), rotation=90)\n",
    "plt.yticks(np.arange(len(vectorC)), np.log10(vectorC))\n",
    "plt.title('5-fold accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Paso 3: mostramos prestaciones en test\n",
    "Copt = grid.best_params_['C']\n",
    "Gopt = grid.best_params_['gamma']\n",
    "\n",
    "svmModel3 = SVC(kernel='rbf',gamma = Gopt, C = Copt, probability=True).fit(X_train,y_train)\n",
    "\n",
    "print('Acc (TEST): %0.2f'%svmModel3.score(X_test,y_test))\n",
    "\n",
    "plot_decision_boundary_svm(X_test,y_test,0.05,svmModel3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Ejemplo realista\n",
    "\n",
    "Vamos a utilizar el mismo conjunto de datos del Notebook anterior, el [Pima Indian Diabetes dataset](https://www.kaggle.com/uciml/pima-indians-diabetes-database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos datos\n",
    "data = pd.read_csv('./data/diabetes.csv',sep=',', decimal='.')\n",
    "\n",
    "# preparamos los datos\n",
    "features = data.columns.drop(['Outcome'])\n",
    "X = data[features].values\n",
    "y = data['Outcome'].values\n",
    "\n",
    "print('Dimensionalidad datos: ', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "**EJERCICIO**: Ajuste un modelo de SVM al conjunto de datos anterior. Devuelva las prestaciones en el conjunto de test y compare con la solución de Boosted Trees.\n",
    "</div>\n",
    "\n",
    "NOTA: Best Test ACC (lo esperado) = 0.794  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paso 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Recursive Feature Elimination\n",
    "\n",
    "Sobre el conjunto anterior, vamos a implementar el algoritmo de selección de características [RFE con validación cruzada](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV), para que casi automáticamente podamos abordar una selección como la realizada en el Notebook 5, sección 1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "svc = SVC(kernel='linear') # ¡sólo funciona con Kernel Lineal!\n",
    "\n",
    "rfecv = RFECV(estimator=svc, step=1, cv=5, scoring='accuracy')\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel(\"# variables\")\n",
    "plt.ylabel(\"5-fold ACC\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_, '-o')\n",
    "plt.show()\n",
    "\n",
    "print('Variables seleccionadas: ',[f for f in features[rfecv.support_]] )\n",
    "print('Acc (TEST): %0.2f'%rfecv.score(X_test,y_test))\n",
    "\n",
    "# ¡No devuelve std! no podemos pintar barras de error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "nfeaturesOptimo = rfecv.n_features_\n",
    "rfe = RFE(estimator=svc, step=1, n_features_to_select= 4).fit(X_train,y_train)\n",
    "print('Variables seleccionadas: ',[f for f in features[rfe.support_]] )\n",
    "\n",
    "print('Acc (TEST): %0.2f'%rfe.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "**EJERCICIO (@home)**: Utiliza un algoritmo de Random Forest para la selección de características\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. SVMs en problemas de regresión\n",
    "\n",
    "El algoritmo SVM en regresión se denomina [SVR](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html).\n",
    "\n",
    "## 2.1 Ejemplo sintético"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 10\n",
    "N_test  = 100\n",
    "\n",
    "# función verdadera g(x)\n",
    "x = np.linspace(0,1,N_test)\n",
    "g_x = np.cos(1.5*np.pi*x)\n",
    "\n",
    "# proceso y\n",
    "np.random.seed(0) # para asegurar reproducibilidad\n",
    "epsilon = np.random.randn(N_test) * 0.2\n",
    "y = g_x + epsilon\n",
    "\n",
    "# Datos: D = {x_i,y_i}, obtenemos una muestra\n",
    "idx = np.random.randint(0,N_test,N_train)\n",
    "x_i = x[idx]\n",
    "y_i = y[idx]\n",
    "\n",
    "# YOUR CODE HERE: dibuje la función g(x), y el conjunto de datos x_i,y_i\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "**EJERCICIO**: Ajusta un modelo SVR para el ejemplo anterior.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# datos\n",
    "X_train = x_i.reshape(-1, 1)\n",
    "y_train = y_i\n",
    "X_test  = x.reshape(-1, 1)\n",
    "\n",
    "# definimos modelo\n",
    "svr = ...\n",
    "\n",
    "# predicción\n",
    "y_hat = svr.predict(X_test)\n",
    "\n",
    "# error test\n",
    "error_test = np.mean(np.power(y - y_hat,2)) \n",
    "\n",
    "# representamos\n",
    "plt.plot(x,g_x,'r',label='$y$')\n",
    "plt.plot(x_i,y_i,'b.',label='$y_i$')\n",
    "plt.plot(x,y_hat,'g',label='$\\hat{y}$')\n",
    "plt.title('MSE:%.2f'%error_test)\n",
    "plt.legend()\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((-2, 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Ejemplo realista en regresión\n",
    "\n",
    "Volvemos a nuestro conjunto de datos ya conocido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos datos\n",
    "house_data = pd.read_csv(\"./data/kc_house_data.csv\") # cargamos fichero\n",
    "\n",
    "# Eliminamos las columnas id y date \n",
    "house_data = house_data.drop(['id','date'], axis=1)\n",
    "\n",
    "# convertir las variables en pies al cuadrado en metros al cuadrado \n",
    "feetFeatures = ['sqft_living','sqft_lot','sqft_above','sqft_basement','sqft_living15','sqft_lot15']\n",
    "house_data[feetFeatures] = house_data[feetFeatures].apply(lambda x: x * 0.3048 * 0.3048)\n",
    "\n",
    "# renombramos\n",
    "house_data.columns = ['price','bedrooms','bathrooms','sqm_living','sqm_lot','floors','waterfront','view','condition',\n",
    "                      'grade','sqm_above','sqm_basement','yr_built','yr_renovated','zip_code','lat','long',\n",
    "                      'sqm_living15','sqm_lot15']\n",
    "\n",
    "# convertimos el DataFrame al formato necesario para scikit-learn\n",
    "data = house_data.values \n",
    "\n",
    "y = data[:,0]     # nos quedamos con la 1ª columna, price\n",
    "X = data[:,1:]      # nos quedamos con el resto\n",
    "\n",
    "feature_names = house_data.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# paso 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, np.log10(y), test_size=.25, random_state = 2)\n",
    "\n",
    "print('Datos entrenamiento: ', X_train.shape)\n",
    "print('Datos test: ', X_test.shape)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "XtrainScaled = scaler.transform(X_train)\n",
    "XtestScaled  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente celda tarda en ejecutarse un tiempo (del orden de 1 hora)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 2:\n",
    "vectorC = np.logspace(-2, 2, 10)\n",
    "vectorG = np.logspace(-5, 1, 8)\n",
    "\n",
    "param_grid = {'C': vectorC, 'gamma':vectorG}\n",
    "grid = GridSearchCV(SVR(kernel='rbf'), param_grid=param_grid, cv = 5, verbose=1)\n",
    "grid.fit(XtrainScaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"best parameters: {}\".format(grid.best_params_))\n",
    "\n",
    "# Mostramos prestaciones en CV\n",
    "scores = grid.cv_results_['mean_test_score'].reshape(len(vectorC),len(vectorG))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(scores, interpolation='nearest', vmin= 0.6, vmax=0.9)\n",
    "plt.xlabel('log(gamma)')\n",
    "plt.ylabel('log(C)')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(vectorG)), np.log10(vectorG), rotation=90)\n",
    "plt.yticks(np.arange(len(vectorC)), np.log10(vectorC))\n",
    "plt.title('5-fold accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3:\n",
    "Copt = grid.best_params_['C']\n",
    "Gopt = grid.best_params_['gamma']\n",
    "\n",
    "svmModel = SVR(kernel='rbf',gamma = Gopt, C = Copt).fit(XtrainScaled,y_train)\n",
    "print('Acc (TEST): %0.2f'%svmModel.score(XtestScaled,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este problema es suficientemente complejo como para ser analizado en una [tesis de máster](http://scholarworks.sjsu.edu/cgi/viewcontent.cgi?article=1540&context=etd_projects)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Otros métodos Kernel\n",
    "\n",
    "## 3.1 Kernel Ridge Regression\n",
    "\n",
    "Vemos un ejemplo sencilo de [Kernel Ridge Regression](http://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html#sklearn.kernel_ridge.KernelRidge). En este caso no es necesario que utilicemos un modelo de datos de alta dimensionalidad, basta elegir adecuadamente los parámetros libres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 10\n",
    "N_test  = 100\n",
    "\n",
    "# función verdadera g(x)\n",
    "x = np.linspace(0,1,N_test)\n",
    "g_x = np.cos(1.5*np.pi*x)\n",
    "\n",
    "# proceso y\n",
    "np.random.seed(0) # para asegurar reproducibilidad\n",
    "epsilon = np.random.randn(N_test) * 0.2\n",
    "y = g_x + epsilon\n",
    "\n",
    "# Datos: D = {x_i,y_i}, obtenemos una muestra\n",
    "idx = np.random.randint(0,N_test,N_train)\n",
    "x_i = x[idx]\n",
    "y_i = y[idx]\n",
    "\n",
    "# YOUR CODE HERE: dibuje la función g(x), y el conjunto de datos x_i,y_i\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "# datos\n",
    "X_train = x_i.reshape(-1, 1)\n",
    "y_train = y_i\n",
    "X_test  = x.reshape(-1, 1)\n",
    "\n",
    "# definimos modelo\n",
    "kRidge = KernelRidge(kernel='rbf',gamma=10,alpha=0.1)\n",
    "kRidge.fit(X_train,y_train)\n",
    "\n",
    "# predicción\n",
    "y_hat = kRidge.predict(X_test)\n",
    "\n",
    "# error test\n",
    "error_test = np.mean(np.power(y - y_hat,2)) \n",
    "\n",
    "# representamos\n",
    "plt.plot(x,g_x,'r',label='$y$')\n",
    "plt.plot(x_i,y_i,'b.',label='$y_i$')\n",
    "plt.plot(x,y_hat,'g',label='$\\hat{y}$')\n",
    "plt.title('MSE:%.2f'%error_test)\n",
    "plt.legend()\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((-2, 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Kernel PCA\n",
    "\n",
    "[Kernel PCA](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.143.2441&rep=rep1&type=pdf) es una reformulación del algoritmo PCA utilizando un kernel, lo que permite aplicar PCA en un espacio de alta dimensionalidad. En vez de calcular los autovectores de la matriz de covarianza, estos se calculan a partir de la matriz de kernel. Y así, las distancias se miden en el espacio de características de alta dimensionalidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "X, y = make_moons(n_samples=100, random_state=123)\n",
    "XKPCA = KernelPCA(n_components=2, kernel='rbf', gamma=15).fit_transform(X)\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "# figure 1\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], color='red', alpha=0.5)\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$x_2$\", fontsize=16)\n",
    "\n",
    "# figure 2\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(XKPCA[y==0, 0], XKPCA[y==0, 1], color='red', alpha=0.5)\n",
    "plt.scatter(XKPCA[y==1, 0], XKPCA[y==1, 1], color='blue', alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$x_2$\", fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "**EJERCICIO**: Aplica Kernel PCA sobre el siguiente ejemplo y representa el resultado\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, y = make_circles(n_samples=1000, random_state=123, noise=0.1, factor=0.2)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "X_kpca = ...\n",
    "\n",
    "####\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], color='red', alpha=0.5)\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], color='blue', alpha=0.5)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$x_2$\", fontsize=16)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "plt.subplot(1,2,2)\n",
    "...\n",
    "####\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
